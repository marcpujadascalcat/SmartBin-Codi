{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "092517cf-44c0-4d7e-949f-b8737e2837be",
   "metadata": {},
   "source": [
    "# âš™ï¸ SmartBin â€” OptimitzaciÃ³ CNN amb Optuna i dataset reorganitzat *(VersiÃ³ 2.2)*\n",
    "\n",
    "> **Aquest notebook ha estat generat amb assistÃ¨ncia dâ€™intelÂ·ligÃ¨ncia artificial, perÃ² totes les decisions sobre les tÃ¨cniques i metodologies utilitzades han estat preses per una persona humana.**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© DescripciÃ³ general\n",
    "\n",
    "Aquesta versiÃ³ del projecte **SmartBin** representa un pas endavant en la millora del sistema de classificaciÃ³ automÃ tica de residus, centrant-se en lâ€™**optimitzaciÃ³ dâ€™una xarxa neuronal convolucional (CNN)** mitjanÃ§ant la llibreria **Optuna**.  \n",
    "El model ha estat adaptat a un **nou conjunt de dades reorganitzat** en format pla, amb quatre categories principals de residus:\n",
    "\n",
    "- **Envasos**  \n",
    "- **Paper**  \n",
    "- **Rebuig**  \n",
    "- **Vidre**\n",
    "\n",
    "Aquesta estructura simplificada permet una gestiÃ³ mÃ©s eficient de les dades i un entrenament mÃ©s coherent del model.\n",
    "\n",
    "> **Dataset utilitzat:** [Recyclable and Household Waste Classification (Kaggle)](https://www.kaggle.com/datasets/alistairking/recyclable-and-household-waste-classification)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Objectiu\n",
    "\n",
    "Lâ€™objectiu principal dâ€™aquesta versiÃ³ Ã©s **millorar la precisiÃ³ i lâ€™equilibri del model** de classificaciÃ³ dâ€™imatges per a la Paperera IntelÂ·ligent (*SmartBin*), aplicant tÃ¨cniques avanÃ§ades com:\n",
    "\n",
    "- **AugmentaciÃ³ dinÃ mica de dades** per incrementar la varietat dâ€™imatges durant lâ€™entrenament.  \n",
    "- **Pesatge de classes (class weighting)** per compensar el desequilibri del dataset.  \n",
    "- **Cerca automÃ tica dâ€™hiperparÃ metres amb Optuna**, garantint una optimitzaciÃ³ mÃ©s eficient i menys dependent dâ€™assaig i error manual.\n",
    "\n",
    "Aquest enfocament busca aconseguir un model robust i adaptable, capaÃ§ de mantenir un rendiment estable fins i tot amb variacions en les condicions de captura dâ€™imatges del SmartBin real.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—‚ï¸ Dataset reorganitzat\n",
    "\n",
    "Les imatges sâ€™han estructurat en un **format pla** per simplificar el flux dâ€™entrenament i evitar jerarquies innecessÃ ries.  \n",
    "Lâ€™organitzaciÃ³ Ã©s la segÃ¼ent:\n",
    "\n",
    "images_reorganitzades/\n",
    "â”œâ”€â”€ envasos/\n",
    "â”œâ”€â”€ paper/\n",
    "â”œâ”€â”€ rebuig/\n",
    "â””â”€â”€ vidre/\n",
    "\n",
    "yaml\n",
    "Copia el codi\n",
    "\n",
    "La divisiÃ³ automÃ tica de dades sâ€™ha realitzat amb la proporciÃ³ estÃ ndard:\n",
    "- **Entrenament:** 60%  \n",
    "- **ValidaciÃ³:** 20%  \n",
    "- **Test:** 20%\n",
    "\n",
    "Per contrarestar possibles desequilibris entre classes (p. ex. mÃ©s imatges dâ€™envasos que de vidre), sâ€™utilitza un **WeightedRandomSampler**, que assegura una representaciÃ³ equilibrada durant lâ€™entrenament.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Arquitectura del model\n",
    "\n",
    "El model utilitzat Ã©s una **xarxa neuronal convolucional lleugera i eficient**, dissenyada per funcionar en entorns fÃ­sics amb recursos limitats, com el SmartBin.\n",
    "\n",
    "| Capa | Tipus | DescripciÃ³ |\n",
    "|------|-------|------------|\n",
    "| **Conv1** | Convolucional | 3â†’32 filtres, *kernel* 3Ã—3, activaciÃ³ ReLU i MaxPooling |\n",
    "| **Conv2** | Convolucional | 32â†’64 filtres, *kernel* 3Ã—3, ReLU i MaxPooling |\n",
    "| **FC1** | Fully Connected | 512 neurones, activaciÃ³ ReLU i Dropout |\n",
    "| **FC2** | Sortida | 4 neurones (una per classe) |\n",
    "\n",
    "Aquesta arquitectura combina senzillesa i potÃ¨ncia, permetent un bon compromÃ­s entre precisiÃ³ i temps de cÃ lcul.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ OptimitzaciÃ³ amb Optuna\n",
    "\n",
    "Per trobar la millor configuraciÃ³ dâ€™hiperparÃ metres, sâ€™ha utilitzat **Optuna**, una llibreria dâ€™optimitzaciÃ³ bayesiana basada en *Tree-structured Parzen Estimators (TPE)*, combinada amb el mÃ¨tode **Median Pruner** per eliminar proves poc prometedores.\n",
    "\n",
    "Durant el procÃ©s, sâ€™han executat **20 proves (trials)** amb lâ€™objectiu de **minimitzar la pÃ¨rdua de validaciÃ³** (*validation loss*).\n",
    "\n",
    "| HiperparÃ metre | Rang de cerca |\n",
    "|-----------------|---------------|\n",
    "| `learning_rate` | 0.0001 â€“ 0.002 |\n",
    "| `batch_size` | 16, 24, 32, 48 |\n",
    "| `num_epochs` | 4 â€“ 10 |\n",
    "| `dropout` | 0.3 â€“ 0.7 |\n",
    "| `weight_factor` | 0.5 â€“ 2.0 |\n",
    "\n",
    "### ğŸ”¬ Altres tÃ¨cniques aplicades\n",
    "- **Early Stopping:** amb una paciÃ¨ncia de 4 Ã¨poques per evitar sobreentrenament.  \n",
    "- **AugmentaciÃ³ dinÃ mica dâ€™imatges:** rotacions, retalls i canvis dâ€™ilÂ·luminaciÃ³ durant lâ€™entrenament.  \n",
    "- **Scheduler automÃ tic del learning rate:** ajusta progressivament la taxa dâ€™aprenentatge per millorar la convergÃ¨ncia.\n",
    "\n",
    "Aquest conjunt de tÃ¨cniques garanteix una optimitzaciÃ³ mÃ©s estable i adaptada a la naturalesa variable del dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea4587e-c2a4-485a-b936-e006fba1cfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ INICIANDO OPTUNA HYPERPARAMETER TUNING - PAPERERA INTELÂ·LIGENT\n",
      "======================================================================\n",
      "ğŸ“‚ Carregant datasets amb estructura reorganitzada...\n",
      "   ğŸ”„ Creant dataset train amb estructura plana...\n",
      "   âœ… Dataset train creat: 9000 imatges\n",
      "   ğŸ“Š DistribuciÃ³ per classe: {'envasos': 3900, 'paper': 1500, 'rebuig': 2700, 'vidre': 900}\n",
      "   ğŸ”„ Creant dataset val amb estructura plana...\n",
      "   âœ… Dataset val creat: 3000 imatges\n",
      "   ğŸ“Š DistribuciÃ³ per classe: {'envasos': 1300, 'paper': 500, 'rebuig': 900, 'vidre': 300}\n",
      "   ğŸ”„ Creant dataset test amb estructura plana...\n",
      "   âœ… Dataset test creat: 3000 imatges\n",
      "   ğŸ“Š DistribuciÃ³ per classe: {'envasos': 1300, 'paper': 500, 'rebuig': 900, 'vidre': 300}\n",
      "ğŸ“Š Dataset: Train=9000, Val=3000, Test=3000 | 4 classes\n",
      "ğŸ·ï¸ Classes: ['envasos', 'paper', 'rebuig', 'vidre']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 12:18:38,896] A new study created in memory with name: waste_classification_optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š GrÃ fic de distribuciÃ³ guardat a: class_distribution.png\n",
      "âš–ï¸ Calculant pesos per equilibrar classes...\n",
      "   ğŸ“ˆ Pesos calculats per 9000 mostres d'entrenament\n",
      "ğŸ¯ Objectiu: Optimitzar classificaciÃ³ de residus amb dataset equilibrat\n",
      "ğŸ’» Device: CUDA\n",
      "âš–ï¸ EstratÃ¨gia: WeightedRandomSampler per equilibrar classes\n",
      "ğŸ”„ AugmentaciÃ³: DinÃ mica nomÃ©s en entrenament\n",
      "â±ï¸ Temps estimat: 3-4 hores per 20 trials\n",
      "ğŸš€ Iniciant optimitzaciÃ³...\n",
      "\n",
      "ğŸ” Trial 1/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00160, bs=48, epochs=5, dropout=0.7, weight_factor=0.7\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.9584(26.2%) | Val=1.3923(10.1%) â¬†ï¸\n",
      "   ğŸ† Â¡NOU RÃˆCORD GLOBAL! Val Loss: 1.392342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.3847(25.0%) | Val=1.3936(10.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.3867(24.4%) | Val=1.3867(16.7%) â¬†ï¸\n",
      "   ğŸ† Â¡NOU RÃˆCORD GLOBAL! Val Loss: 1.386658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 4: Train=1.3865(25.0%) | Val=1.3870(16.7%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 12:25:34,857] Trial 0 finished with value: 1.3866584130695887 and parameters: {'learning_rate': 0.001602351229389994, 'batch_size': 48, 'num_epochs': 5, 'dropout': 0.7, 'weight_factor': 0.7}. Best is trial 0 with value: 1.3866584130695887.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 5: Train=1.3867(24.6%) | Val=1.3897(10.0%)\n",
      "   âœ… COMPLETAT - Millor Val Loss: 1.386658\n",
      "\n",
      "ğŸ” Trial 2/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00055, bs=16, epochs=9, dropout=0.4, weight_factor=1.7\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.4516(42.7%) | Val=1.1887(41.4%) â¬†ï¸\n",
      "   ğŸ† Â¡NOU RÃˆCORD GLOBAL! Val Loss: 1.188687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.1643(49.1%) | Val=1.1914(41.4%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.1158(52.2%) | Val=1.1611(43.5%) â¬†ï¸\n",
      "   ğŸ† Â¡NOU RÃˆCORD GLOBAL! Val Loss: 1.161063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 4: Train=1.0937(53.2%) | Val=1.0272(55.9%) â¬†ï¸\n",
      "   ğŸ† Â¡NOU RÃˆCORD GLOBAL! Val Loss: 1.027181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 5: Train=1.0567(55.2%) | Val=1.0487(51.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 6: Train=1.0334(56.6%) | Val=1.0323(53.9%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 7: Train=1.0090(57.6%) | Val=1.0958(51.4%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 8: Train=1.0002(58.5%) | Val=1.0118(54.7%) â¬†ï¸\n",
      "   ğŸ† Â¡NOU RÃˆCORD GLOBAL! Val Loss: 1.011800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 12:37:53,915] Trial 1 finished with value: 1.011800361282014 and parameters: {'learning_rate': 0.0005510798882853513, 'batch_size': 16, 'num_epochs': 9, 'dropout': 0.4, 'weight_factor': 1.7000000000000002}. Best is trial 1 with value: 1.011800361282014.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 9: Train=0.9610(60.3%) | Val=1.0517(53.9%)\n",
      "   âœ… COMPLETAT - Millor Val Loss: 1.011800\n",
      "\n",
      "ğŸ” Trial 3/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00013, bs=48, epochs=4, dropout=0.3, weight_factor=1.0\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.4392(41.0%) | Val=1.1893(45.5%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.1807(48.0%) | Val=1.1303(51.1%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.1466(50.7%) | Val=1.2692(37.5%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 12:43:12,493] Trial 2 finished with value: 1.0860327756594097 and parameters: {'learning_rate': 0.00013087668316424247, 'batch_size': 48, 'num_epochs': 4, 'dropout': 0.3, 'weight_factor': 1.0}. Best is trial 1 with value: 1.011800361282014.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 4: Train=1.0937(53.8%) | Val=1.0860(52.8%) â¬†ï¸\n",
      "   âœ… COMPLETAT - Millor Val Loss: 1.086033\n",
      "\n",
      "ğŸ” Trial 4/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00011, bs=32, epochs=10, dropout=0.6, weight_factor=1.5\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.3819(40.3%) | Val=1.2062(44.9%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.1995(47.1%) | Val=1.2071(41.5%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.1520(50.1%) | Val=1.0972(51.0%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 4: Train=1.1096(52.7%) | Val=1.0573(53.1%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 5: Train=1.0931(53.5%) | Val=1.0835(49.9%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 6: Train=1.0581(54.7%) | Val=1.0302(53.5%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 7: Train=1.0467(55.8%) | Val=1.1133(49.9%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 12:53:43,357] Trial 3 pruned.                                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 8: Train=1.0425(55.8%) | Val=1.0869(49.6%)\n",
      "   âœ‚ï¸ Trial podat - no Ã©s prometedor\n",
      "\n",
      "ğŸ” Trial 5/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00081, bs=16, epochs=10, dropout=0.4, weight_factor=0.6\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.6692(33.3%) | Val=1.3313(33.0%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.3230(37.5%) | Val=1.2564(41.9%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.2864(40.1%) | Val=1.2641(39.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 12:59:11,662] Trial 4 pruned.                                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 4: Train=1.2549(41.9%) | Val=1.3339(30.6%)\n",
      "   âœ‚ï¸ Trial podat - no Ã©s prometedor\n",
      "\n",
      "ğŸ” Trial 6/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00046, bs=24, epochs=8, dropout=0.5, weight_factor=2.0\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.5503(38.8%) | Val=1.2649(38.4%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.1713(49.0%) | Val=1.2644(38.0%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.1129(52.0%) | Val=1.1371(47.2%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 4: Train=1.0722(55.1%) | Val=1.0700(53.3%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 5: Train=1.0510(54.6%) | Val=1.1638(46.9%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 13:07:55,442] Trial 5 pruned.                                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 6: Train=1.0240(57.0%) | Val=1.0379(54.7%) â¬†ï¸\n",
      "   âœ‚ï¸ Trial podat - no Ã©s prometedor\n",
      "\n",
      "ğŸ” Trial 7/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00028, bs=16, epochs=7, dropout=0.3, weight_factor=1.7\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.3859(42.2%) | Val=1.2092(43.5%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.1484(49.6%) | Val=1.1628(46.0%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.0740(53.2%) | Val=1.0821(50.5%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 4: Train=1.0420(55.7%) | Val=1.0742(52.9%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 5: Train=1.0082(58.2%) | Val=1.0332(54.3%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 6: Train=0.9848(58.5%) | Val=0.9938(56.5%) â¬†ï¸\n",
      "   ğŸ† Â¡NOU RÃˆCORD GLOBAL! Val Loss: 0.993817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 13:17:42,519] Trial 6 finished with value: 0.9938167148131005 and parameters: {'learning_rate': 0.00027946436514684163, 'batch_size': 16, 'num_epochs': 7, 'dropout': 0.3, 'weight_factor': 1.7000000000000002}. Best is trial 6 with value: 0.9938167148131005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 7: Train=0.9518(60.6%) | Val=1.0019(57.9%)\n",
      "   âœ… COMPLETAT - Millor Val Loss: 0.993817\n",
      "\n",
      "ğŸ” Trial 8/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00024, bs=16, epochs=6, dropout=0.3, weight_factor=1.3\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.3397(46.4%) | Val=1.1364(47.8%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.1035(52.5%) | Val=1.0363(54.9%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.0583(55.0%) | Val=1.0368(53.9%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 4: Train=1.0212(56.5%) | Val=0.9777(60.3%) â¬†ï¸\n",
      "   ğŸ† Â¡NOU RÃˆCORD GLOBAL! Val Loss: 0.977747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 5: Train=0.9878(59.0%) | Val=0.9271(60.1%) â¬†ï¸\n",
      "   ğŸ† Â¡NOU RÃˆCORD GLOBAL! Val Loss: 0.927139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 13:26:05,874] Trial 7 finished with value: 0.9271392457662745 and parameters: {'learning_rate': 0.00023725365531016801, 'batch_size': 16, 'num_epochs': 6, 'dropout': 0.3, 'weight_factor': 1.3}. Best is trial 7 with value: 0.9271392457662745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 6: Train=0.9434(61.2%) | Val=1.0674(54.2%)\n",
      "   âœ… COMPLETAT - Millor Val Loss: 0.927139\n",
      "\n",
      "ğŸ” Trial 9/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00022, bs=32, epochs=6, dropout=0.5, weight_factor=1.1\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.3908(43.3%) | Val=1.2651(39.6%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.1473(49.9%) | Val=1.0961(52.2%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.0904(53.3%) | Val=1.1104(49.7%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 13:31:32,117] Trial 8 pruned.                                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 4: Train=1.0555(54.8%) | Val=1.0792(51.7%) â¬†ï¸\n",
      "   âœ‚ï¸ Trial podat - no Ã©s prometedor\n",
      "\n",
      "ğŸ” Trial 10/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00024, bs=24, epochs=6, dropout=0.3, weight_factor=1.2\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.4980(42.6%) | Val=1.1606(48.3%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.1540(49.8%) | Val=1.2069(43.4%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.0864(53.7%) | Val=1.0192(55.0%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 4: Train=1.0506(55.3%) | Val=1.1013(50.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 5: Train=1.0324(56.5%) | Val=1.0462(53.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 13:39:46,655] Trial 9 finished with value: 0.9726516058444977 and parameters: {'learning_rate': 0.00023571430945231906, 'batch_size': 24, 'num_epochs': 6, 'dropout': 0.3, 'weight_factor': 1.2000000000000002}. Best is trial 7 with value: 0.9271392457662745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 6: Train=0.9848(58.5%) | Val=0.9727(58.8%) â¬†ï¸\n",
      "   âœ… COMPLETAT - Millor Val Loss: 0.972652\n",
      "\n",
      "ğŸ” Trial 11/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00116, bs=16, epochs=4, dropout=0.4, weight_factor=0.9\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.6198(33.1%) | Val=1.3282(24.7%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.3045(36.1%) | Val=1.2926(26.7%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 13:44:08,319] Trial 10 pruned.                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.2785(37.5%) | Val=1.2687(27.1%) â¬†ï¸\n",
      "   âœ‚ï¸ Trial podat - no Ã©s prometedor\n",
      "\n",
      "ğŸ” Trial 12/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00025, bs=24, epochs=6, dropout=0.3, weight_factor=1.3\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.3963(44.3%) | Val=1.1698(45.2%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.1223(51.3%) | Val=1.1667(45.5%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.0798(53.9%) | Val=1.0980(51.2%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 13:49:42,567] Trial 11 pruned.                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 4: Train=1.0443(55.0%) | Val=1.1491(47.0%)\n",
      "   âœ‚ï¸ Trial podat - no Ã©s prometedor\n",
      "\n",
      "ğŸ” Trial 13/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00018, bs=24, epochs=6, dropout=0.3, weight_factor=1.3\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.4265(43.7%) | Val=1.1469(47.3%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.1479(50.3%) | Val=1.1040(48.5%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.0822(53.2%) | Val=1.0512(52.2%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 4: Train=1.0611(54.7%) | Val=1.0624(49.7%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 5: Train=1.0304(56.0%) | Val=0.9999(54.5%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 13:57:58,841] Trial 12 finished with value: 0.9999280292987823 and parameters: {'learning_rate': 0.00018404360437965353, 'batch_size': 24, 'num_epochs': 6, 'dropout': 0.3, 'weight_factor': 1.3}. Best is trial 7 with value: 0.9271392457662745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 6: Train=1.0062(57.7%) | Val=1.0273(53.1%)\n",
      "   âœ… COMPLETAT - Millor Val Loss: 0.999928\n",
      "\n",
      "ğŸ” Trial 14/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00034, bs=24, epochs=7, dropout=0.4, weight_factor=1.5\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.5730(43.2%) | Val=1.2089(39.9%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.1498(50.2%) | Val=1.2007(44.5%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:02:12,500] Trial 13 pruned.                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.1081(52.6%) | Val=1.1367(47.3%) â¬†ï¸\n",
      "   âœ‚ï¸ Trial podat - no Ã©s prometedor\n",
      "\n",
      "ğŸ” Trial 15/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00015, bs=16, epochs=5, dropout=0.6, weight_factor=1.2\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.3212(43.1%) | Val=1.2516(39.0%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.1657(49.6%) | Val=1.1165(49.5%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:06:24,622] Trial 14 pruned.                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.1029(52.9%) | Val=1.1139(49.9%) â¬†ï¸\n",
      "   âœ‚ï¸ Trial podat - no Ã©s prometedor\n",
      "\n",
      "ğŸ” Trial 16/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00035, bs=24, epochs=8, dropout=0.3, weight_factor=0.8\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.4033(43.3%) | Val=1.1980(42.8%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.1319(50.8%) | Val=1.1780(45.4%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.0654(55.0%) | Val=0.9953(57.3%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 4: Train=1.0021(57.0%) | Val=1.0740(51.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 5: Train=1.0109(57.6%) | Val=1.1325(49.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 6: Train=0.9468(60.9%) | Val=0.9265(59.4%) â¬†ï¸\n",
      "   ğŸ† Â¡NOU RÃˆCORD GLOBAL! Val Loss: 0.926481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 7: Train=0.9454(60.3%) | Val=0.8809(63.1%) â¬†ï¸\n",
      "   ğŸ† Â¡NOU RÃˆCORD GLOBAL! Val Loss: 0.880915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:17:29,631] Trial 15 finished with value: 0.8809148006439209 and parameters: {'learning_rate': 0.0003454842518803514, 'batch_size': 24, 'num_epochs': 8, 'dropout': 0.3, 'weight_factor': 0.8}. Best is trial 15 with value: 0.8809148006439209.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 8: Train=0.8922(63.6%) | Val=0.9667(57.8%)\n",
      "   âœ… COMPLETAT - Millor Val Loss: 0.880915\n",
      "\n",
      "ğŸ” Trial 17/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00063, bs=48, epochs=8, dropout=0.4, weight_factor=0.8\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.9378(31.8%) | Val=1.3244(35.7%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.3055(38.7%) | Val=1.3731(30.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:21:39,855] Trial 16 pruned.                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.2126(45.9%) | Val=1.2779(36.1%) â¬†ï¸\n",
      "   âœ‚ï¸ Trial podat - no Ã©s prometedor\n",
      "\n",
      "ğŸ” Trial 18/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00036, bs=32, epochs=8, dropout=0.5, weight_factor=0.5\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.5639(39.9%) | Val=1.1687(47.1%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.1940(47.8%) | Val=1.1022(51.5%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.1177(51.6%) | Val=1.0545(53.5%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 4: Train=1.0836(53.9%) | Val=1.0270(53.5%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 5: Train=1.0467(55.7%) | Val=1.0947(50.8%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 6: Train=1.0324(55.9%) | Val=0.9548(59.5%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 7: Train=1.0042(58.0%) | Val=0.9440(58.9%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:32:20,333] Trial 17 finished with value: 0.8920124109755171 and parameters: {'learning_rate': 0.00036117313241027454, 'batch_size': 32, 'num_epochs': 8, 'dropout': 0.5, 'weight_factor': 0.5}. Best is trial 15 with value: 0.8809148006439209.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 8: Train=0.9899(58.6%) | Val=0.8920(63.1%) â¬†ï¸\n",
      "   âœ… COMPLETAT - Millor Val Loss: 0.892012\n",
      "\n",
      "ğŸ” Trial 19/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00039, bs=32, epochs=8, dropout=0.6, weight_factor=0.5\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.5836(38.1%) | Val=1.2675(34.8%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.1661(49.2%) | Val=1.1151(49.8%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:36:23,712] Trial 18 pruned.                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.1235(52.1%) | Val=1.1230(48.3%)\n",
      "   âœ‚ï¸ Trial podat - no Ã©s prometedor\n",
      "\n",
      "ğŸ” Trial 20/20\n",
      "   ğŸ“Š ParÃ metres: lr=0.00086, bs=32, epochs=9, dropout=0.7, weight_factor=0.5\n",
      "   ğŸ¯ Baseline a superar: validation loss anterior\n",
      "   ğŸš€ Iniciant entrenament amb WeightedSampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 1: Train=1.9290(31.4%) | Val=1.3844(19.8%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 2: Train=1.3430(35.0%) | Val=1.3058(34.8%) â¬†ï¸\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-01 14:40:24,293] Trial 19 pruned.                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 3: Train=1.3259(36.7%) | Val=1.3436(27.2%)\n",
      "   âœ‚ï¸ Trial podat - no Ã©s prometedor\n",
      "\n",
      "======================================================================\n",
      "ğŸ‰ OPTIMITZACIÃ“ COMPLETADA - PAPERERA INTELÂ·LIGENT\n",
      "======================================================================\n",
      "ğŸ† MILLOR RESULTAT:\n",
      "   ğŸ“Š Validation Loss: 0.880915\n",
      "   âš™ï¸ ParÃ metres: {'learning_rate': 0.0003454842518803514, 'batch_size': 24, 'num_epochs': 8, 'dropout': 0.3, 'weight_factor': 0.8}\n",
      "   ğŸ”¢ Trial nÃºmero: 15\n",
      "\n",
      "ğŸ“ˆ Resum de trials:\n",
      "   âœ… Trials completats: 20\n",
      "   âœ‚ï¸ Trials podats: 11\n",
      "   âŒ Trials fallits: 0\n",
      "\n",
      "ğŸ§ª AVALUACIÃ“ EN TEST SET:\n",
      "--------------------------------------------------\n",
      "ğŸ” Evaluant model en dataset de test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AvaluaciÃ³: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 94/94 [00:17<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Test Loss: 0.855474\n",
      "ğŸ¯ Test Accuracy: 64.23%\n",
      "\n",
      "ğŸ“‹ ACCURACY PER CLASSE:\n",
      "   envasos : P=0.751 R=0.628 F1=0.684\n",
      "   paper   : P=0.613 R=0.720 F1=0.662\n",
      "   rebuig  : P=0.618 R=0.629 F1=0.623\n",
      "   vidre   : P=0.451 R=0.617 F1=0.521\n",
      "\n",
      "ğŸ’¾ GENERANT INFORMES:\n",
      "--------------------------------------------------\n",
      "ğŸ“Š Matriu de confusiÃ³ guardada a: paperera_confusion_matrix.png\n",
      "ğŸ“„ Informe complet guardat a: paperera_optimization_report.json\n",
      "ğŸ“ˆ GrÃ fic d'optimitzaciÃ³ guardat a: paperera_optimization_history.png\n",
      "âœ… Arxius generats:\n",
      "   ğŸ“¦ paperera_model_final_acc64.2.pth - Millor model optimitzat\n",
      "   ğŸ”§ paperera_model_config.json - ConfiguraciÃ³ del model\n",
      "   ğŸ“Š paperera_confusion_matrix.png - Matriu de confusiÃ³\n",
      "   ğŸ“„ paperera_optimization_report.json - Informe complet\n",
      "   ğŸ“ˆ paperera_optimization_history.png - HistÃ²ria d'optimitzaciÃ³\n",
      "   ğŸ“Š class_distribution.png - DistribuciÃ³ de classes\n",
      "\n",
      "ğŸŠ Â¡PROCÃ‰S COMPLETAT EXITOSAMENT!\n",
      "\n",
      "ğŸ’¡ RECOMANACIONS PER LA PAPERERA INTELÂ·LIGENT:\n",
      "--------------------------------------------------\n",
      "âœ… Model optimitzat amb WeightedSampler per equilibrar classes\n",
      "âœ… AugmentaciÃ³ dinÃ mica aplicada per millorar generalitzaciÃ³\n",
      "âœ… Dataset reorganitzat en 4 categories principals de residus\n",
      "ğŸ” Classe amb millor rendiment: envasos (F1: 0.684)\n",
      "âš ï¸ Classe a millorar: vidre (F1: 0.521)\n",
      "\n",
      "ğŸš€ SEGÃœENTS PASSOS:\n",
      "1. Integrar model en sistema fÃ­sic de la paperera\n",
      "2. Provar amb imatges reals capturades per la cÃ mera\n",
      "3. Ajustar threshold de confianÃ§a per decisions\n",
      "4. Implementar sistema de feedback per millorar model\n",
      "\n",
      "ğŸ“‹ ESTADÃSTIQUES DETALLADES:\n",
      "--------------------------------------------------\n",
      "ğŸ“Š Total mostres test: 3000\n",
      "âœ… Prediccions correctes: 1927\n",
      "âŒ Prediccions incorrectes: 1073\n",
      "ğŸ¯ Accuracy global: 64.23%\n",
      "\n",
      "ğŸ”¢ MATRIU DE CONFUSIÃ“ (format text):\n",
      "       envasos     paper    rebuig     vidre\n",
      " envasos      816      107      223      154\n",
      "   paper       53      360       78        9\n",
      "  rebuig      161      111      566       62\n",
      "   vidre       57        9       49      185\n",
      "\n",
      "======================================================================\n",
      "ğŸ“‹ INFORMACIÃ“ DEL SISTEMA DE PAPERERA INTELÂ·LIGENT\n",
      "======================================================================\n",
      "ğŸ—‚ï¸ Classes de residus:\n",
      "   0: envasos\n",
      "   1: paper\n",
      "   2: rebuig\n",
      "   3: vidre\n",
      "\n",
      "ğŸ”§ ConfiguraciÃ³ utilitzada:\n",
      "   ğŸ“ Estructura dataset: Plana reorganitzada\n",
      "   âš–ï¸ Sampler: WeightedRandomSampler per equilibrar\n",
      "   ğŸ”„ AugmentaciÃ³: NomÃ©s en entrenament\n",
      "   ğŸ“Š Split: 60% train / 20% val / 20% test\n",
      "   ğŸ¯ OptimitzaciÃ³: TPE + MedianPruner\n",
      "\n",
      "ğŸ FIN DEL PROCÃ‰S D'OPTIMITZACIÃ“\n",
      "ğŸ¤– Model llest per integrar en la paperera intelÂ·ligent!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Dataset Class Millorada per Dataset Pla\n",
    "class WasteDatasetFlat(Dataset):\n",
    "    def __init__(self, root_dir, split, transform=None, val_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.val_transform = val_transform  # Transformacions sense augmentaciÃ³ per val/test\n",
    "        self.split = split\n",
    "        \n",
    "        # Classes finals reorganitzades\n",
    "        self.classes = ['envasos', 'paper', 'rebuig', 'vidre']\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        \n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        print(f\"   ğŸ”„ Creant dataset {split} amb estructura plana...\")\n",
    "        \n",
    "        # Carregar imatges de la estructura plana reorganitzada\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if not os.path.exists(class_dir):\n",
    "                print(f\"   âš ï¸ AdvertÃ¨ncia: No es troba la carpeta {class_dir}\")\n",
    "                continue\n",
    "                \n",
    "            # Obtenir totes les imatges de la classe\n",
    "            image_names = []\n",
    "            for file in os.listdir(class_dir):\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    image_names.append(file)\n",
    "            \n",
    "            # Barrejar per obtenir splits consistents\n",
    "            random.seed(42)  # Seed fix per reproducibilitat\n",
    "            random.shuffle(image_names)\n",
    "            \n",
    "            # Dividir segons split (60/20/20)\n",
    "            total_images = len(image_names)\n",
    "            train_end = int(0.6 * total_images)\n",
    "            val_end = int(0.8 * total_images)\n",
    "            \n",
    "            if split == 'train':\n",
    "                selected_images = image_names[:train_end]\n",
    "            elif split == 'val':\n",
    "                selected_images = image_names[train_end:val_end]\n",
    "            else:  # test\n",
    "                selected_images = image_names[val_end:]\n",
    "            \n",
    "            # Afegir al dataset\n",
    "            for image_name in selected_images:\n",
    "                full_path = os.path.join(class_dir, image_name)\n",
    "                self.image_paths.append(full_path)\n",
    "                self.labels.append(self.class_to_idx[class_name])\n",
    "        \n",
    "        # EstadÃ­stiques del dataset\n",
    "        label_counts = Counter(self.labels)\n",
    "        class_distribution = {self.classes[i]: label_counts[i] for i in range(len(self.classes))}\n",
    "        \n",
    "        print(f\"   âœ… Dataset {split} creat: {len(self.image_paths)} imatges\")\n",
    "        print(f\"   ğŸ“Š DistribuciÃ³ per classe: {class_distribution}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error carregant {image_path}: {e}\")\n",
    "            # Retornar una imatge negra com a fallback\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        \n",
    "        # Aplicar transformacions segons el split\n",
    "        if self.transform and self.split == 'train':\n",
    "            image = self.transform(image)\n",
    "        elif self.val_transform and self.split in ['val', 'test']:\n",
    "            image = self.val_transform(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def get_class_weights(self):\n",
    "        \"\"\"Calcula els pesos per WeightedRandomSampler\"\"\"\n",
    "        label_counts = Counter(self.labels)\n",
    "        total_samples = len(self.labels)\n",
    "        \n",
    "        # Calcular pesos inversament proporcionals a la freqÃ¼Ã¨ncia\n",
    "        class_weights = []\n",
    "        for i in range(len(self.classes)):\n",
    "            weight = total_samples / (len(self.classes) * label_counts[i])\n",
    "            class_weights.append(weight)\n",
    "        \n",
    "        # Crear pesos per cada mostra\n",
    "        sample_weights = [class_weights[label] for label in self.labels]\n",
    "        \n",
    "        return torch.DoubleTensor(sample_weights)\n",
    "\n",
    "# CNN Model (mantenint el mateix que tenies)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device, class_names):\n",
    "    \"\"\"EvalÃºa el modelo y retorna mÃ©tricas detalladas\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    print(\"ğŸ” Evaluant model en dataset de test...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"AvaluaciÃ³\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_loss = total_loss / len(test_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return {\n",
    "        'test_loss': test_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': all_preds,\n",
    "        'true_labels': all_labels,\n",
    "        'classification_report': classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, save_path='confusion_matrix.png'):\n",
    "    \"\"\"Genera i guarda la matriu de confusiÃ³\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Matriu de ConfusiÃ³ - Millor Model', fontsize=16)\n",
    "    plt.ylabel('Etiquetes Reals', fontsize=12)\n",
    "    plt.xlabel('Prediccions', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"ğŸ“Š Matriu de confusiÃ³ guardada a: {save_path}\")\n",
    "\n",
    "def generate_report(study, best_metrics, class_names, save_path='optimization_report.json'):\n",
    "    \"\"\"Genera un informe complet de l'optimitzaciÃ³\"\"\"\n",
    "    report = {\n",
    "        'optimization_info': {\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'project': 'Paperera IntelÂ·ligent - ClassificaciÃ³ de Residus',\n",
    "            'dataset_info': {\n",
    "                'classes': class_names,\n",
    "                'total_classes': len(class_names),\n",
    "                'data_split': '60/20/20 (train/val/test)'\n",
    "            },\n",
    "            'n_trials': len(study.trials),\n",
    "            'best_trial_number': study.best_trial.number,\n",
    "            'optimization_direction': 'minimize_validation_loss'\n",
    "        },\n",
    "        'best_hyperparameters': study.best_trial.params,\n",
    "        'best_validation_loss': study.best_trial.value,\n",
    "        'test_metrics': {\n",
    "            'test_loss': best_metrics['test_loss'],\n",
    "            'test_accuracy': best_metrics['accuracy']\n",
    "        },\n",
    "        'classification_report': best_metrics['classification_report'],\n",
    "        'trial_history': [\n",
    "            {\n",
    "                'trial': t.number,\n",
    "                'params': t.params,\n",
    "                'value': t.value,\n",
    "                'state': str(t.state)\n",
    "            } for t in study.trials if t.value is not None\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"ğŸ“„ Informe complet guardat a: {save_path}\")\n",
    "    return report\n",
    "\n",
    "def plot_optimization_history(study, save_path='optimization_history.png'):\n",
    "    \"\"\"Genera grÃ fic de la histÃ²ria d'optimitzaciÃ³\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # HistÃ²ria de valors\n",
    "    trial_numbers = [t.number for t in study.trials if t.value is not None]\n",
    "    values = [t.value for t in study.trials if t.value is not None]\n",
    "    \n",
    "    ax1.plot(trial_numbers, values, 'b-o', markersize=4)\n",
    "    ax1.axhline(y=study.best_value, color='r', linestyle='--', label=f'Millor: {study.best_value:.4f}')\n",
    "    ax1.set_xlabel('Trial')\n",
    "    ax1.set_ylabel('Validation Loss')\n",
    "    ax1.set_title('HistÃ²ria d\\'OptimitzaciÃ³')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # DistribuciÃ³ de valors\n",
    "    if len(values) > 0:\n",
    "        ax2.hist(values, bins=min(20, len(values)), alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax2.axvline(x=study.best_value, color='r', linestyle='--', label=f'Millor: {study.best_value:.4f}')\n",
    "    ax2.set_xlabel('Validation Loss')\n",
    "    ax2.set_ylabel('FreqÃ¼Ã¨ncia')\n",
    "    ax2.set_title('DistribuciÃ³ de Resultats')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"ğŸ“ˆ GrÃ fic d'optimitzaciÃ³ guardat a: {save_path}\")\n",
    "\n",
    "def plot_class_distribution(train_dataset, save_path='class_distribution.png'):\n",
    "    \"\"\"Genera grÃ fic de distribuciÃ³ de classes\"\"\"\n",
    "    label_counts = Counter(train_dataset.labels)\n",
    "    class_names = train_dataset.classes\n",
    "    counts = [label_counts[i] for i in range(len(class_names))]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(class_names, counts, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "    \n",
    "    # Afegir valors sobre les barres\n",
    "    for bar, count in zip(bars, counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.title('DistribuciÃ³ de Classes en Dataset d\\'Entrenament', fontsize=14)\n",
    "    plt.xlabel('Classes de Residus')\n",
    "    plt.ylabel('Nombre d\\'Imatges')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"ğŸ“Š GrÃ fic de distribuciÃ³ guardat a: {save_path}\")\n",
    "\n",
    "# ConfiguraciÃ³\n",
    "print(\"ğŸš€ INICIANDO OPTUNA HYPERPARAMETER TUNING - PAPERERA INTELÂ·LIGENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Transformacions amb augmentaciÃ³ dinÃ mica per entrenament\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((240, 240)),  # MÃ©s gran per desprÃ©s fer crop\n",
    "    transforms.RandomCrop((224, 224)),  # Crop aleatori\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Transformacions sense augmentaciÃ³ per validaciÃ³ i test\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Inicialitzar datasets\n",
    "print(\"ğŸ“‚ Carregant datasets amb estructura reorganitzada...\")\n",
    "dataset_path = 'images_reorganitzades'  # Nova ruta amb estructura plana\n",
    "\n",
    "train_dataset = WasteDatasetFlat(dataset_path, 'train', \n",
    "                                transform=train_transform, \n",
    "                                val_transform=val_test_transform)\n",
    "val_dataset = WasteDatasetFlat(dataset_path, 'val', \n",
    "                              val_transform=val_test_transform)\n",
    "test_dataset = WasteDatasetFlat(dataset_path, 'test', \n",
    "                               val_transform=val_test_transform)\n",
    "\n",
    "print(f\"ğŸ“Š Dataset: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)} | {len(train_dataset.classes)} classes\")\n",
    "print(f\"ğŸ·ï¸ Classes: {train_dataset.classes}\")\n",
    "\n",
    "# Generar grÃ fic de distribuciÃ³\n",
    "plot_class_distribution(train_dataset)\n",
    "\n",
    "# Calcular pesos per WeightedRandomSampler\n",
    "print(\"âš–ï¸ Calculant pesos per equilibrar classes...\")\n",
    "sample_weights = train_dataset.get_class_weights()\n",
    "print(f\"   ğŸ“ˆ Pesos calculats per {len(sample_weights)} mostres d'entrenament\")\n",
    "\n",
    "# Variables globals per tracking\n",
    "best_global_loss = float('inf')\n",
    "best_model_state = None\n",
    "trial_count = 0\n",
    "\n",
    "def objective(trial):\n",
    "    global best_global_loss, best_model_state, trial_count\n",
    "    trial_count += 1\n",
    "    \n",
    "    # HiperparÃ metres optimitzats per dataset desequilibrat\n",
    "    lr = trial.suggest_float('learning_rate', 0.0001, 0.002, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 24, 32, 48])\n",
    "    epochs = trial.suggest_int('num_epochs', 4, 10)\n",
    "    dropout = trial.suggest_float('dropout', 0.3, 0.7, step=0.1)\n",
    "    \n",
    "    # Nou parÃ metre: factor de pes per classes\n",
    "    weight_factor = trial.suggest_float('weight_factor', 0.5, 2.0, step=0.1)\n",
    "    \n",
    "    print(f\"\\nğŸ” Trial {trial_count}/20\")\n",
    "    print(f\"   ğŸ“Š ParÃ metres: lr={lr:.5f}, bs={batch_size}, epochs={epochs}, dropout={dropout:.1f}, weight_factor={weight_factor:.1f}\")\n",
    "    print(f\"   ğŸ¯ Baseline a superar: validation loss anterior\")\n",
    "    \n",
    "    # Crear DataLoaders amb WeightedRandomSampler per train\n",
    "    # Aplicar factor de pes als sample weights\n",
    "    adjusted_weights = sample_weights * weight_factor\n",
    "    \n",
    "    weighted_sampler = WeightedRandomSampler(\n",
    "        weights=adjusted_weights,\n",
    "        num_samples=len(adjusted_weights),\n",
    "        replacement=True  # Permetre repeticions per oversampling\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        sampler=weighted_sampler,  # Usar sampler en lloc de shuffle\n",
    "        num_workers=0, \n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=0, \n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # Model setup amb dropout personalitzable\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = CNN(len(train_dataset.classes), dropout=dropout).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 4  # Augmentem paciÃ¨ncia per models mÃ©s complexos\n",
    "    \n",
    "    print(f\"   ğŸš€ Iniciant entrenament amb WeightedSampler...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        train_bar = tqdm(train_loader, desc=f\"   Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "        for images, labels in train_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Actualitzar barra de progrÃ©s\n",
    "            current_train_loss = train_loss / (train_bar.n + 1)\n",
    "            current_train_acc = 100. * train_correct / train_total\n",
    "            train_bar.set_postfix({'Loss': f'{current_train_loss:.4f}', 'Acc': f'{current_train_acc:.1f}%'})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = 100. * train_correct / train_total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(val_loader, desc=f\"   Epoch {epoch+1}/{epochs} [Val]  \", leave=False)\n",
    "            for images, labels in val_bar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                # Actualitzar barra de progrÃ©s\n",
    "                current_val_loss = val_loss / (val_bar.n + 1)\n",
    "                current_val_acc = 100. * val_correct / val_total\n",
    "                val_bar.set_postfix({'Loss': f'{current_val_loss:.4f}', 'Acc': f'{current_val_acc:.1f}%'})\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100. * val_correct / val_total\n",
    "        \n",
    "        # Mostrar resultats de l'epoch\n",
    "        improvement_indicator = \"\"\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            improvement_indicator = \" â¬†ï¸\"\n",
    "        \n",
    "        print(f\"   Epoch {epoch+1}: Train={avg_train_loss:.4f}({train_accuracy:.1f}%) | Val={avg_val_loss:.4f}({val_accuracy:.1f}%){improvement_indicator}\")\n",
    "        \n",
    "        # Report intermediate result per pruning\n",
    "        trial.report(avg_val_loss, epoch)\n",
    "        \n",
    "        # Prune si no Ã©s prometedor\n",
    "        if trial.should_prune():\n",
    "            print(f\"   âœ‚ï¸ Trial podat - no Ã©s prometedor\")\n",
    "            raise optuna.TrialPruned()\n",
    "        \n",
    "        # Update best validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Si Ã©s el millor model global, guardar-lo\n",
    "            if avg_val_loss < best_global_loss:\n",
    "                best_global_loss = avg_val_loss\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                torch.save(best_model_state, 'best_model_temp.pth')\n",
    "                print(f\"   ğŸ† Â¡NOU RÃˆCORD GLOBAL! Val Loss: {avg_val_loss:.6f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"   â° Early stopping (sense millora per {patience} epochs)\")\n",
    "            break\n",
    "    \n",
    "    print(f\"   âœ… COMPLETAT - Millor Val Loss: {best_val_loss:.6f}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, train_loader, val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "# Crear estudi d'Optuna\n",
    "print(f\"ğŸ¯ Objectiu: Optimitzar classificaciÃ³ de residus amb dataset equilibrat\")\n",
    "print(f\"ğŸ’» Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"âš–ï¸ EstratÃ¨gia: WeightedRandomSampler per equilibrar classes\")\n",
    "print(f\"ğŸ”„ AugmentaciÃ³: DinÃ mica nomÃ©s en entrenament\")\n",
    "print(f\"â±ï¸ Temps estimat: 3-4 hores per 20 trials\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    study_name='waste_classification_optimization',\n",
    "    sampler=optuna.samplers.TPESampler(n_startup_trials=5),\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=2)\n",
    ")\n",
    "\n",
    "# Executar optimitzaciÃ³\n",
    "print(\"ğŸš€ Iniciant optimitzaciÃ³...\")\n",
    "try:\n",
    "    study.optimize(objective, n_trials=20, timeout=14400)  # 20 trials, 4h max\n",
    "    optimization_completed = True\n",
    "except KeyboardInterrupt:\n",
    "    print(\"â¸ï¸ Interromput per usuari\")\n",
    "    optimization_completed = True\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error durant optimitzaciÃ³: {e}\")\n",
    "    optimization_completed = False\n",
    "\n",
    "# Resultats finals\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ‰ OPTIMITZACIÃ“ COMPLETADA - PAPERERA INTELÂ·LIGENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(study.trials) > 0 and study.best_trial:\n",
    "    print(f\"ğŸ† MILLOR RESULTAT:\")\n",
    "    print(f\"   ğŸ“Š Validation Loss: {study.best_trial.value:.6f}\")\n",
    "    print(f\"   âš™ï¸ ParÃ metres: {study.best_trial.params}\")\n",
    "    print(f\"   ğŸ”¢ Trial nÃºmero: {study.best_trial.number}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Resum de trials:\")\n",
    "    print(f\"   âœ… Trials completats: {len([t for t in study.trials if t.value is not None])}\")\n",
    "    print(f\"   âœ‚ï¸ Trials podats: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}\")\n",
    "    print(f\"   âŒ Trials fallits: {len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL])}\")\n",
    "    \n",
    "    # Evaluar el millor model en test set\n",
    "    if os.path.exists('best_model_temp.pth'):\n",
    "        print(f\"\\nğŸ§ª AVALUACIÃ“ EN TEST SET:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        best_model = CNN(len(train_dataset.classes), dropout=study.best_trial.params.get('dropout', 0.5)).to(device)\n",
    "        best_model.load_state_dict(torch.load('best_model_temp.pth'))\n",
    "        \n",
    "        # Crear test loader sense weighted sampler\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        test_metrics = evaluate_model(best_model, test_loader, criterion, device, train_dataset.classes)\n",
    "        \n",
    "        print(f\"ğŸ“Š Test Loss: {test_metrics['test_loss']:.6f}\")\n",
    "        print(f\"ğŸ¯ Test Accuracy: {test_metrics['accuracy']*100:.2f}%\")\n",
    "        \n",
    "        # Mostrar accuracy per classe\n",
    "        print(f\"\\nğŸ“‹ ACCURACY PER CLASSE:\")\n",
    "        class_report = test_metrics['classification_report']\n",
    "        for class_name in train_dataset.classes:\n",
    "            if class_name in class_report:\n",
    "                precision = class_report[class_name]['precision']\n",
    "                recall = class_report[class_name]['recall']\n",
    "                f1 = class_report[class_name]['f1-score']\n",
    "                print(f\"   {class_name:8}: P={precision:.3f} R={recall:.3f} F1={f1:.3f}\")\n",
    "        \n",
    "        # Generar tots els informes i grÃ fics\n",
    "        print(f\"\\nğŸ’¾ GENERANT INFORMES:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        plot_confusion_matrix(test_metrics['true_labels'], test_metrics['predictions'], \n",
    "                            train_dataset.classes, 'paperera_confusion_matrix.png')\n",
    "        \n",
    "        generate_report(study, test_metrics, train_dataset.classes, 'paperera_optimization_report.json')\n",
    "        \n",
    "        plot_optimization_history(study, 'paperera_optimization_history.png')\n",
    "        \n",
    "        # Guardar model final amb nom descriptiu\n",
    "        model_filename = f\"paperera_model_final_acc{test_metrics['accuracy']*100:.1f}.pth\"\n",
    "        torch.save(best_model.state_dict(), model_filename)\n",
    "        \n",
    "        # Guardar tambÃ© configuraciÃ³ del model\n",
    "        model_config = {\n",
    "            'num_classes': len(train_dataset.classes),\n",
    "            'classes': train_dataset.classes,\n",
    "            'dropout': study.best_trial.params.get('dropout', 0.5),\n",
    "            'best_hyperparameters': study.best_trial.params,\n",
    "            'test_accuracy': test_metrics['accuracy'],\n",
    "            'test_loss': test_metrics['test_loss'],\n",
    "            'model_architecture': 'CNN_2Conv_2FC'\n",
    "        }\n",
    "        \n",
    "        with open('paperera_model_config.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(model_config, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(\"âœ… Arxius generats:\")\n",
    "        print(f\"   ğŸ“¦ {model_filename} - Millor model optimitzat\")\n",
    "        print(\"   ğŸ”§ paperera_model_config.json - ConfiguraciÃ³ del model\")\n",
    "        print(\"   ğŸ“Š paperera_confusion_matrix.png - Matriu de confusiÃ³\")\n",
    "        print(\"   ğŸ“„ paperera_optimization_report.json - Informe complet\")\n",
    "        print(\"   ğŸ“ˆ paperera_optimization_history.png - HistÃ²ria d'optimitzaciÃ³\")\n",
    "        print(\"   ğŸ“Š class_distribution.png - DistribuciÃ³ de classes\")\n",
    "        \n",
    "        # Netejar arxiu temporal\n",
    "        if os.path.exists('best_model_temp.pth'):\n",
    "            os.remove('best_model_temp.pth')\n",
    "        \n",
    "        print(f\"\\nğŸŠ Â¡PROCÃ‰S COMPLETAT EXITOSAMENT!\")\n",
    "        \n",
    "        # Mostrar recomanacions per la paperera\n",
    "        print(f\"\\nğŸ’¡ RECOMANACIONS PER LA PAPERERA INTELÂ·LIGENT:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(\"âœ… Model optimitzat amb WeightedSampler per equilibrar classes\")\n",
    "        print(\"âœ… AugmentaciÃ³ dinÃ mica aplicada per millorar generalitzaciÃ³\")\n",
    "        print(\"âœ… Dataset reorganitzat en 4 categories principals de residus\")\n",
    "        \n",
    "        # AnÃ lisi de rendiment per categoria\n",
    "        worst_class = min(class_report.keys(), \n",
    "                         key=lambda x: class_report[x]['f1-score'] if x in train_dataset.classes else 1.0)\n",
    "        best_class = max(class_report.keys(), \n",
    "                        key=lambda x: class_report[x]['f1-score'] if x in train_dataset.classes else 0.0)\n",
    "        \n",
    "        if worst_class in train_dataset.classes and best_class in train_dataset.classes:\n",
    "            print(f\"ğŸ” Classe amb millor rendiment: {best_class} (F1: {class_report[best_class]['f1-score']:.3f})\")\n",
    "            print(f\"âš ï¸ Classe a millorar: {worst_class} (F1: {class_report[worst_class]['f1-score']:.3f})\")\n",
    "        \n",
    "        print(\"\\nğŸš€ SEGÃœENTS PASSOS:\")\n",
    "        print(\"1. Integrar model en sistema fÃ­sic de la paperera\")\n",
    "        print(\"2. Provar amb imatges reals capturades per la cÃ mera\")\n",
    "        print(\"3. Ajustar threshold de confianÃ§a per decisions\")\n",
    "        print(\"4. Implementar sistema de feedback per millorar model\")\n",
    "        \n",
    "        # EstadÃ­stiques detallades per al desenvolupador\n",
    "        print(f\"\\nğŸ“‹ ESTADÃSTIQUES DETALLADES:\")\n",
    "        print(\"-\" * 50)\n",
    "        total_test_samples = len(test_metrics['true_labels'])\n",
    "        correct_predictions = sum(1 for true, pred in zip(test_metrics['true_labels'], test_metrics['predictions']) if true == pred)\n",
    "        print(f\"ğŸ“Š Total mostres test: {total_test_samples}\")\n",
    "        print(f\"âœ… Prediccions correctes: {correct_predictions}\")\n",
    "        print(f\"âŒ Prediccions incorrectes: {total_test_samples - correct_predictions}\")\n",
    "        print(f\"ğŸ¯ Accuracy global: {test_metrics['accuracy']*100:.2f}%\")\n",
    "        \n",
    "        # Mostrar matriu de confusiÃ³ en text\n",
    "        cm = confusion_matrix(test_metrics['true_labels'], test_metrics['predictions'])\n",
    "        print(f\"\\nğŸ”¢ MATRIU DE CONFUSIÃ“ (format text):\")\n",
    "        print(\"     \", \"  \".join([f\"{cls[:8]:>8}\" for cls in train_dataset.classes]))\n",
    "        for i, class_name in enumerate(train_dataset.classes):\n",
    "            row = \" \".join([f\"{cm[i][j]:8d}\" for j in range(len(train_dataset.classes))])\n",
    "            print(f\"{class_name[:8]:>8} {row}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"âš ï¸ No es va trobar el model guardat\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ No es van completar trials exitosos\")\n",
    "    print(\"ğŸ”§ Verifica:\")\n",
    "    print(\"   - Ruta del dataset: 'images_reorganitzades/'\")\n",
    "    print(\"   - Estructura: images_reorganitzades/[classe]/[imatges]\")\n",
    "    print(\"   - MemÃ²ria GPU disponible\")\n",
    "    print(\"   - Format d'imatges (PNG/JPG)\")\n",
    "\n",
    "# InformaciÃ³ addicional sobre el sistema\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“‹ INFORMACIÃ“ DEL SISTEMA DE PAPERERA INTELÂ·LIGENT\")\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ—‚ï¸ Classes de residus:\")\n",
    "for i, class_name in enumerate(['envasos', 'paper', 'rebuig', 'vidre']):\n",
    "    print(f\"   {i}: {class_name}\")\n",
    "\n",
    "print(\"\\nğŸ”§ ConfiguraciÃ³ utilitzada:\")\n",
    "print(\"   ğŸ“ Estructura dataset: Plana reorganitzada\")\n",
    "print(\"   âš–ï¸ Sampler: WeightedRandomSampler per equilibrar\")\n",
    "print(\"   ğŸ”„ AugmentaciÃ³: NomÃ©s en entrenament\")\n",
    "print(\"   ğŸ“Š Split: 60% train / 20% val / 20% test\")\n",
    "print(\"   ğŸ¯ OptimitzaciÃ³: TPE + MedianPruner\")\n",
    "\n",
    "print(f\"\\nğŸ FIN DEL PROCÃ‰S D'OPTIMITZACIÃ“\")\n",
    "print(\"ğŸ¤– Model llest per integrar en la paperera intelÂ·ligent!\")\n",
    "\n",
    "# Cleanup final\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac9b1fe-9b55-431c-86c3-1b72a0c5eded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
